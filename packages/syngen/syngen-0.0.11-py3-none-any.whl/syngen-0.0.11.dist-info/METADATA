Metadata-Version: 2.1
Name: syngen
Version: 0.0.11
Summary: The tool uncovers patterns, trends, and correlations hidden within your production datasets.
Home-page: https://github.com/tdspora/syngen
Author: EPAM Systems, Inc.
Maintainer: Pavel Bobyrev
License: GPLv3 License
Keywords: data,generation,synthetic,vae,tabular
Classifier: Development Status :: 5 - Production/Stable
Classifier: Operating System :: POSIX :: Linux
Classifier: Operating System :: Microsoft :: Windows
Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Requires-Python: <3.9,>3.6
Description-Content-Type: text/markdown
License-File: LICENCE.txt
License-File: LICENSE
Requires-Dist: absl-py (==0.12.0)
Requires-Dist: astunparse (==1.6.3)
Requires-Dist: attrs (==21.4.0)
Requires-Dist: cachetools (==5.2.0)
Requires-Dist: category-encoders (==2.5.0)
Requires-Dist: certifi (==2022.6.15)
Requires-Dist: charset-normalizer (==2.1.0)
Requires-Dist: click (==8.1.3)
Requires-Dist: cloudpickle (==2.1.0)
Requires-Dist: colorama (==0.4.5)
Requires-Dist: cycler (==0.11.0)
Requires-Dist: decorator (==5.1.1)
Requires-Dist: dill (==0.3.5.1)
Requires-Dist: dm-tree (==0.1.7)
Requires-Dist: etils[epath] (==0.6.0)
Requires-Dist: fastavro (==1.5.1)
Requires-Dist: flatbuffers (==2.0)
Requires-Dist: fonttools (==4.34.4)
Requires-Dist: gast (==0.5.3)
Requires-Dist: google-auth (==2.9.1)
Requires-Dist: google-auth-oauthlib (==0.4.6)
Requires-Dist: google-pasta (==0.2.0)
Requires-Dist: googleapis-common-protos (==1.56.4)
Requires-Dist: grpcio (==1.47.0)
Requires-Dist: h5py (==3.7.0)
Requires-Dist: idna (==3.3)
Requires-Dist: importlib-metadata (==4.12.0)
Requires-Dist: importlib-resources (==5.8.0)
Requires-Dist: joblib (==1.1.0)
Requires-Dist: keras (==2.8.0)
Requires-Dist: keras-preprocessing (==1.1.2)
Requires-Dist: kiwisolver (==1.4.4)
Requires-Dist: lazy (==1.4)
Requires-Dist: libclang (==14.0.1)
Requires-Dist: loguru (==0.6.0)
Requires-Dist: markdown (==3.4.1)
Requires-Dist: matplotlib (==3.5.2)
Requires-Dist: mpmath (==1.2.1)
Requires-Dist: multiprocess (==0.70.13)
Requires-Dist: names-dataset (==1.9.0)
Requires-Dist: nltk (==3.7)
Requires-Dist: numpy (==1.20.0)
Requires-Dist: oauthlib (==3.2.0)
Requires-Dist: opt-einsum (==3.3.0)
Requires-Dist: packaging (==21.3)
Requires-Dist: pandas (==1.3.3)
Requires-Dist: pandavro (==1.7.1)
Requires-Dist: pathos (==0.2.9)
Requires-Dist: patsy (==0.5.2)
Requires-Dist: pillow (==9.2.0)
Requires-Dist: pox (==0.3.1)
Requires-Dist: ppft (==1.7.6.5)
Requires-Dist: promise (==2.3)
Requires-Dist: protobuf (==3.20.1)
Requires-Dist: pyasn1 (==0.4.8)
Requires-Dist: pyasn1-modules (==0.2.8)
Requires-Dist: pyparsing (==3.0.9)
Requires-Dist: python-dateutil (==2.8.2)
Requires-Dist: pytz (==2022.1)
Requires-Dist: regex (==2022.7.9)
Requires-Dist: requests (==2.28.1)
Requires-Dist: requests-oauthlib (==1.3.1)
Requires-Dist: rsa (==4.9)
Requires-Dist: scikit-learn (==0.23.2)
Requires-Dist: scipy (==1.7.1)
Requires-Dist: seaborn (==0.11.2)
Requires-Dist: six (==1.16.0)
Requires-Dist: statsmodels (==0.13.2)
Requires-Dist: tensorboard (==2.8.0)
Requires-Dist: tensorboard-data-server (==0.6.1)
Requires-Dist: tensorboard-plugin-wit (==1.8.1)
Requires-Dist: tensorflow (==2.8.0)
Requires-Dist: tensorflow-datasets (==4.6.0)
Requires-Dist: tensorflow-estimator (==2.7.0)
Requires-Dist: tensorflow-io-gcs-filesystem (==0.26.0)
Requires-Dist: tensorflow-metadata (==1.9.0)
Requires-Dist: tensorflow-privacy (==0.7.3)
Requires-Dist: tensorflow-probability (==0.16.0)
Requires-Dist: termcolor (==1.1.0)
Requires-Dist: tf-estimator-nightly (==2.8.0.dev2021122109)
Requires-Dist: threadpoolctl (==3.1.0)
Requires-Dist: toml (==0.10.2)
Requires-Dist: tqdm (==4.64.0)
Requires-Dist: typing-extensions (==4.3.0)
Requires-Dist: urllib3 (==1.26.10)
Requires-Dist: werkzeug (==2.1.2)
Requires-Dist: wheel (==0.37.1)
Requires-Dist: win32-setctime (==1.1.0)
Requires-Dist: wrapt (==1.14.1)
Requires-Dist: zipp (==3.8.1)

![Build Status](https://github.com/tdspora/syngen/workflows/TESTING/badge.svg)
# Syngen

Syngen is an unsupervised tabular data generation tool. It is useful for generation of test data with a given table as a template. Most datatypes including floats, integers, datetime, text, categorical, binary are supported. The linked tables i.e., tables sharing a key can also be generated using the simple statistical approach.

The tool is based on the variational autoencoder model (VAE). The Bayesian Gaussian Mixture model is used to further detangle the latent space.

## Getting started

Use pip to install the library:

`pip install syngen`

The training and inference processes are separated with two cli entry points. The training one receives paths to the original table, metadata json file or table name and used hyperparameters. To start training with the sensible defaults run

`train PATH_TO_ORIGINAL_CSV –table_name TABLE_NAME`

This will train a model and save the model artifacts to disk.

To generate data simply call

`infer SIZE TABLE_NAME`

This will create a csv file with the synthetic table in ./model_artifacts/tmp_store/TABLE_NAME/merged_infer.csv

Here is a quick example:

```
pip install syngen
train ./data/Churn_modelling.csv –table_name Churn
infer 5000 Churn
```

## Features

### Training

You can add flexibility to the training and inference processes using additional hyperparameters.

`train PATH_TO_ORIGINAL_CSV –metadata_path PATH_TO_METADATA_JSON –table_name TABLE_NAME –epochs INT –row_limit INT –dropna BOOL –keys_mode BOOL`

- PATH_TO_ORIGINAL_CSV – a path to the csv table that you want to use a reference
- metadata_path – a path to the json file containing the metadata (see below)
- table_name – an arbitrary string to name the directories. If table name is provided and `–keys_mode` is False the `–metadata_path` argument is optional
- epochs – the number of training epochs. Since the early stopping mechanism is implemented the bigger is the better
- row_limit – the number of rows to train over. A number less then the original table length will randomly subset the specified rows number
- dropna – whether to drop rows with at least one missing value
- keys_mode – whether to train linked tables (see below)


### Inference

You can customize the inference processes by calling

`infer SIZE TABLE_NAME –run_parallel BOOL –batch_size INT –keys_mode BOOL –metadata_path PATH_TO_METADATA –random_seed INT- --print_report BOOL`

- SIZE - the desired number of rows to generate
- TABLE_NAME – the name of the table, same as in training
- run_parallel – whether to use multiprocessing (feasible for tables > 5000 rows)
- batch_size – if specified, the generation is split into batches. This can save the RAM
- keys_mode – whether to generate linked tables (see below)
- metadata_path – a path to metadata json file. If `--keys mode` is set to False the argument is optional
- random_seed – if specified, generates a reproducible result
- print_report – whether to generate plots of pairwise distributions, accuracy matrix and print the median accuracy


### Linked tables generation

To generate linked tables, you need to train tables in the special order:

A table with the Primary key (training) -> a table with the Primary key (inference) -> a table with the foreign key (training) -> a table with the foreign key (inference)

You have to set `--keys_mode` to True in every step and provide the metadata for the Foreign key table training and inference as a json file with the following structure:

`{"table_name": "NAME_OF_FK_TABLE", "fk": {"NAME_OF_FK_COLUMN": {"pk_table": "NAME_OF_PK_TABLE", "pk_column": "NAME_OF_PK_COLUMN (in PK table)"}}}`

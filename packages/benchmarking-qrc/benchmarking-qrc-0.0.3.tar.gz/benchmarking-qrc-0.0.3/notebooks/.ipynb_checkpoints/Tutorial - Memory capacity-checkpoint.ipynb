{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c00837d",
   "metadata": {},
   "source": [
    "# Tutorial - Quantum Reservoir Computing\n",
    "\n",
    "The main goal of this notebook is to familiarize you with Quantum Reservoir Computing (QRC). \n",
    "\n",
    "In this tutorial you will learn:\n",
    "- What is QRC?\n",
    "- How you can train a QRC to perform a machine learning task?\n",
    "- Evaluate the performance for different particles\n",
    "\n",
    "This tutorial is based on the paper [_Benchmarking the role of particle statistics in Quantum Reservoir Computing_](https://ifisc.uib-csic.es/es/).\n",
    "<font color='red'> Put arxiv link </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf2af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "import benchmarking_qrc as qrc\n",
    "from python_scripts import memory_capacity as mc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b5bc6",
   "metadata": {},
   "source": [
    "## Introduction - What is QRC?\n",
    "\n",
    "\n",
    "Quantum Reservoir Computing (QRC) is based on a machine learning technique called Reservoir Computing (RC). RC is specially suited to solve temporal tasks (ex: historical power consumtion, annual temperatures, stock market) because it exploits the dynamics of a nonlinear system called reservoir. The intuition behind RC is to map an input into a high-dimensional space (reservoir) that facilittes the separtion of the initial data. After this mapping, the output layer is trained to extract valuable reservoir features and mathc the expected output.\n",
    "\n",
    "There are other machine learning techniques like Recurrent Neural Networks, which its dynamics also exploits the fact that the information can remain stored in the network for a finite amount of time. This characteristic is often defined as memory, making RNN and RC suitable to process sequential data. However, RNN feedback loops (see Figure) can be extremely hard to train. In constrat, RC only train the weights in the output layer, which leads to an easy training and fast learning because backpropagation is not need to optimize weights.\n",
    "\n",
    "<img src=\"images/rnn_and_rc.png\" width=\"700\" height=\"300\">\n",
    "\n",
    "As shown in the above Figure the architecture of RNN looks rather complex compare to RC with only three layers. To ensure a proper performance the reservoir must satisfy several requirements:  \n",
    "- **Convergence** or **Echo State Property**: The result obtained in the output layer must be totally independent of the initial conditions of the reservoir.\n",
    "- **Nonlinearity**: The reservoir must provide a nonlinear transformation with respect to the inputs. In this way, most of the computational cost can be outsourced to the reservoir while the output layer can be taken as a linear funtion, which its easier to train. \n",
    "<font color='red'> Ho he agafat de es paper de Rodrigo, posar-ho a ses references </font>\n",
    "- **Fading memory**: The dynamics in the reservoir is able to retain memory of present and past inputs, but it also able to dissipate the information for irrelevant inputs very far in the past.\n",
    "- **Separability**: Different inputs sequence should produce different outputs.\n",
    "\n",
    "All these features (complex dynamics and high-dimensional space) can be enhanced if we move to the field of Quantum Reservoir Computing (QRC). A major advantatge lies in the fact that quantum computers can store exponentially more information in the Hilbert space, where quantum particles reside, than classical computers. In addition, QRC is suitable for Noisy intermediate-scale quantum (NISQ) since it doesn't need perfect dynamics to perform computation. As shown, in the seminal work of Nakajima et. al <font color='red'> ref Nakajima 2017 paper </font>, a QRC of 5-7 qubits exhibits similar performance to an echo state network (classical method) of 100-500 nodes.\n",
    "\n",
    "The standard algorithm of RC/QRC can be explained in three steps as shown in the following Figure:\n",
    "- **Input**: Injecting data into an ancilla particle, which is coupled to the reservoir. In this step, the low-dimensional input data is mapped into a high-dimensional dynamical system. \n",
    "- **Reservoir**: Let the reservoir evolve for a certain period of time following its natural dynamics. At each iteration, a new input will be injected driving the dynamics. \n",
    "- **Output**: Extract features from the reservoir. Then, a linear combination of all the features will be optimized to reproduce the desired target. <font color='red'> Cambiar la f de Readout por observables </font>\n",
    "\n",
    "In the next section, we will introduce the mathematical formalism describing each step.\n",
    "\n",
    "<img src=\"images/QRC_diagram.png\" width=\"600\" height=\"200\">\n",
    "\n",
    "Although different types of systems have been proposed as quantum reservoirs, the role of the statistics has not been established yet. In this work, we have assessed the ability of different particles (fermions, bosons and spins) to store information from past inputs. This has allowed us to find two key ingredients to take into account for future QRC projects. Firstly, fermions are better information carriers than spins due to the anticommutation rules. Secondly, under a tailored input injection strategy bosons can exploit the abundance of degrees of freedom in its Hilbert space and improve its ability to store information. For more details, check out the article in this link <font color='red'> Put arxiv link/or github link to the paper. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a407c-0b35-4dfd-b876-42ac48cabbbb",
   "metadata": {},
   "source": [
    "### The task\n",
    "\n",
    "In this section, we will briefly introduce the machine learning task that the QRC is going to perform and the metric to quantify the results.\n",
    "\n",
    "Our objective is to quantify the how well the quantum system can recover past information. So, we will inject a sequence of random inputs ($u_k$) into the reservoir and the system will be trained to match a target function\n",
    "\n",
    "$$ \\hat{y}_k = u_{k-\\tau}$$\n",
    "\n",
    "where $\\tau$ is the delay between the input and target.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Example:</b>    \n",
    "    \n",
    "Given an input sequence:\n",
    "$$ u = (0.2, 0.3, 0.4, 0.5) $$\n",
    "\n",
    "The target function after a delay, $\\tau=1$ is:\n",
    "\n",
    "$$ \\hat{y} = (0.1, 0.2, 0.3, 0.4) $$\n",
    "\n",
    "But the reservoir after the training procedure may output a different result:\n",
    "\n",
    "$$ y = (0.11, 0.19, 0.3, 0.398) $$\n",
    "</div>\n",
    "\n",
    "Let us introduce the metric to meause the system memory. The memory capacity is a standard measure of memory in recurrent neural network and is defined as:\n",
    "\n",
    "$$ MC = \\frac{Cov^2(y,\\hat{y})}{\\sigma^2(y)\\sigma^2(\\hat{y})} $$\n",
    "\n",
    "where $\\sigma(y)$ and $\\sigma(\\hat{y})$ are the standard deviation of the reservoir and target outputs, respectively. The $MC$ may look familiar to some readers because it is simply the Pearson correlation coefficient to the power 2. This implies that the MC is bounded between 0 (there is no correlation between $y$ and $\\hat{y}$) and 1 (the system has been able to fully recover the target function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced6201",
   "metadata": {},
   "source": [
    "## Hands-on approach - How to train a QRC \n",
    "\n",
    "In this section we explain the mathematical formalism to train a Quantum Reservoir with a fermionic reservoir of only two particles to visualize what the code is actually doing. Later on, we will increase the reservoir size up to 4 fermions. The following table summarizes all the equations that we are going to introduce:\n",
    "\n",
    "\n",
    "<img src=\"images/table.png\" width=\"350\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee07344-018c-4ca4-b225-c11548a247ff",
   "metadata": {},
   "source": [
    "### Step 0: Setting up the system\n",
    " \n",
    "\n",
    "The first step is to define the reservoir initial conditions, namely define a the initial state of a quantum many-body system. In this case the reservoir is build with 2 fermions in the ground state $\\lvert 0 \\rangle$ using the function `dm_zero_states`. Since fermions are a two-level systems ($\\lvert 0 \\rangle$, $\\lvert 1 \\rangle$) the Hilbert dimensions of fermions is always 2 (`dim_fermion=2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036ecdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial parameters\n",
    "n_fermions = 2\n",
    "dim_fermion = 2\n",
    "\n",
    "initial_reservoir = qrc.reservoir.initial_state(n_fermions, dim_fermion)\n",
    "initial_reservoir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbfa3a5",
   "metadata": {},
   "source": [
    "To process information particles must interact with each other. The interaction strenght between particles $i$ and $j$ is defined by the parameter $J_{ij}$ and the natural dynamics of the system is ruled by the following quadratic Hamiltonian\n",
    "\n",
    "$$ H = \\sum_{i,j=1}^{N}J_{ij}a_{i}^{\\dagger}a_{j} $$\n",
    "\n",
    "This Hamiltonian encodes the onsite interation ($J_{ii}$) and the coupling iterations ($J_{ij}$) among all particles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cd72bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.  +0.j, 0.  +0.j, 0.  +0.j, 0.  +0.j],\n",
       "        [0.  +0.j, 0.72+0.j, 0.  +0.j, 0.  +0.j],\n",
       "        [0.  +0.j, 0.  +0.j, 0.42+0.j, 0.  +0.j],\n",
       "        [0.  +0.j, 0.  +0.j, 0.  +0.j, 1.14+0.j]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generates random values between [0, 1)\n",
    "J_ij = qrc.hamiltonian.get_coefficients(n_fermions, coef_range=[0, 1], seed=1) \n",
    "\n",
    "fermionic_hamiltonian = qrc.hamiltonian.quadratic_op(n_fermions, is_bosonic=False, dimensions=dim_fermion, coefficients=J_ij)\n",
    "\n",
    "np.around(fermionic_hamiltonian.todense(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ffb56",
   "metadata": {},
   "source": [
    "As a last ingredient to simulate the dynamics of the reservoir, we need the evolution operator ($e^{-iH\\Delta t}$) to compute the next state of reservoir ($\\rho(t)$). \n",
    "\n",
    "$$ \\rho(t_f) = e^{-iH\\Delta t}\\rho(t_i)e^{iH\\Delta t}$$\n",
    "\n",
    "The $\\Delta t$ quantifies how much time we let system evolve. By default, $\\Delta t = 10$ because we want that the injected information in the system is spread through the whole system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca89bda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.  +0.j  ,  0.  +0.j  ,  0.  +0.j  ,  0.  +0.j  ],\n",
       "       [ 0.  +0.j  ,  0.61-0.8j ,  0.  -0.j  ,  0.  +0.j  ],\n",
       "       [ 0.  +0.j  ,  0.  -0.j  , -0.52+0.86j,  0.  +0.j  ],\n",
       "       [ 0.  +0.j  ,  0.  +0.j  ,  0.  +0.j  ,  0.37+0.93j]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fermionic_evolution = qrc.hamiltonian.get_evolution_op(fermionic_hamiltonian, dt=10)\n",
    "np.around(fermionic_evolution, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ce1d5",
   "metadata": {},
   "source": [
    "### Step 1: Echo state property (wash out)\n",
    "\n",
    "To avoid any possible effect of the initial conditions of the reservoir we always start with the wash out step before training the system. This step consists in injecting a set of inputs until the initial conditions become irrelevant. The amount of iterations needed to wash out the initial conditions can be found in Figure 2 of the paper.\n",
    "\n",
    "To encode classical data ($u_k$) into a quantum state ($\\lvert \\psi \\rangle$) we can use `get_input_state`.\n",
    "\n",
    "$$ \\lvert \\psi_k^{(e=1)} \\rangle = \\sqrt{u_k}\\lvert 0 \\rangle + \\sqrt{1-u_k}\\lvert 1 \\rangle$$\n",
    "\n",
    "For fermions we need to inject 3000 inputs to erase the initial conditions according to the Figure 2. However, it is possible that for certain values of $J_{ij}$ the numbers of iterations required is lower. Notice that for fermions the excited level is obvioulsy $\\lvert 1 \\rangle$, but for bosons the excited level could be $\\lvert 1 \\rangle$, $\\lvert 2 \\rangle$ ... $\\lvert n \\rangle$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8501e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54984777],\n",
       "       [0.83526488]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wash_time = 3000\n",
    "wash_out_signals = np.random.uniform(low=0, high=1, size=wash_time)\n",
    "\n",
    "# Generating the first input state\n",
    "excited_state=1\n",
    "input_state = qrc.reservoir.get_input_state(wash_out_signals[0], dim_fermion, excited_state)\n",
    "input_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de60c716",
   "metadata": {},
   "source": [
    "Then, we can inject this quantum state into the first particle of the reservoir with `insert_input`. This function computes the tensor product between the input state, $\\rho_{1, k}^{(e)}=\\lvert \\psi_k^{(e)} \\rangle \\langle \\psi_k^{(e)} \\lvert$, and all particles of the reservoir except the first one, ${\\rm Tr}_{1}\\left\\{ \\rho\\left(\\left(k-1\\right)\\Delta t\\right)\\right\\}$\n",
    "\n",
    "$$ \\rho(k\\Delta t)= \\rho_{1, k}^{(e)} \\otimes {\\rm Tr}_{1}\\left\\{ \\rho\\left(\\left(k-1\\right)\\Delta t\\right)\\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15da2cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30233257, 0.        , 0.45926854, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.45926854, 0.        , 0.69766743, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reservoir_with_input = qrc.reservoir.insert_input(input_state, initial_reservoir)\n",
    "reservoir_with_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7127e4",
   "metadata": {},
   "source": [
    "To let the information spread through the whole system we let the system evolve following the quantum dynamics of a closed system, as shown in the table above.\n",
    "\n",
    "\n",
    "$$ \\rho\\left(k\\Delta t\\right)=e^{-iH\\Delta t}\\left[\\rho_{1,k}^{(e)}\\otimes {\\rm Tr}_{1}\\left\\{ \\rho\\left(\\left(k-1\\right)\\Delta t\\right)\\right\\} \\right]e^{iH\\Delta t} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458d8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir = qrc.reservoir.evolve(reservoir_with_input, fermionic_evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430a0a2",
   "metadata": {},
   "source": [
    "So, at each iteration the reservoir will change following these 3 steps:\n",
    "- Encoding classical data into a quantum state $\\Longrightarrow \\lvert \\psi_k^{(e)} \\rangle$.\n",
    "- Injecting the quantum state into the reservoir $\\Longrightarrow \\rho(k)$.\n",
    "- Evolving the reservoir $\\Longrightarrow  \\rho(k\\Delta t)$.\n",
    "\n",
    "To avoid calling the previous three functions at each iteration, we have packed the previous 3 points in a single function call `cptp_map` which will generate a reservoir state for each injected input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d6fc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.191+0.j   , -0.   +0.j   , -0.203-0.337j,  0.   +0.001j],\n",
       "       [-0.   -0.j   ,  0.   +0.j   ,  0.   +0.001j, -0.   -0.j   ],\n",
       "       [-0.203+0.337j,  0.   -0.001j,  0.808+0.j   , -0.002-0.001j],\n",
       "       [ 0.   -0.001j, -0.   +0.j   , -0.002+0.001j,  0.   +0.j   ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generates a list with the state of the reservoir after each iteration\n",
    "reservoirs = list(qrc.reservoir.cptp_map(wash_out_signals, initial_reservoir, fermionic_evolution, dim_fermion, excited_state))\n",
    "\n",
    "# Take the last reservoir from the previous list\n",
    "washed_reservoir = reservoirs[-1]\n",
    "np.around(washed_reservoir, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5f16f",
   "metadata": {},
   "source": [
    "### Step 2: Data prepartion\n",
    "\n",
    "Once we have a `washed_reservoir` you can start training the reservoir to perform any typical supervise machine learning task. In a real world scenario you will have a dataset with a time-series (ex: temperatures over several years, stock market, etc.).\n",
    "\n",
    "In our case, we are interested in benchmarking the ability to store past inputs, so a random sequence of number is good enough for our porpuses. Let's start by setting some parameters, as the `delay`, `train_time` and `test_time`.\n",
    "- `delay`: Number of iterations between the input and target data. As the delay increase, it will become harder to remember the input.\n",
    "- `train_time` and `test_time`: Number of iterations to train/test the reservoir.\n",
    "\n",
    "Then, instead of uploading a dataset we will simply generate random numbers. \n",
    "_Remember that we have already set a seed for generating the coefficients $J_{ij}$, so the following results are totally reproducible_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f102d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = 1\n",
    "train_time, test_time = 10, 10\n",
    "\n",
    "# Prepare random data\n",
    "train_signals = np.random.uniform(low=0, high=1, size=train_time + delay)\n",
    "test_signals = np.random.uniform(low=0, high=1, size=test_time + delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405454f",
   "metadata": {},
   "source": [
    "Let's also prepare the input with the function `get_inputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea016e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs = [0.96 0.56 0.65 0.27 0.95 0.67 0.67 0.01 0.85 0.78 0.74 0.16 0.72 0.1\n",
      " 0.71 0.   0.78 0.49 0.2  0.73]\n"
     ]
    }
   ],
   "source": [
    "# Generating inputs to train and test the reservoir.\n",
    "train_inputs = mc.get_inputs(train_signals, delay)\n",
    "test_inputs = mc.get_inputs(test_signals, delay)\n",
    "inputs = np.concatenate((train_inputs, test_inputs))\n",
    "print(\"inputs =\", np.around(inputs, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b23cbd",
   "metadata": {},
   "source": [
    "As well, the function `get_targets` generates the target for linear task when $\\hat{y}=u_k$ and for the non-linear task $\\hat{y}=u_k^q$ ($q \\neq 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cef6d00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_targets = [0.98 0.96 0.56 0.65 0.27 0.95 0.67 0.67 0.01 0.85]\n"
     ]
    }
   ],
   "source": [
    "# Generating targets\n",
    "train_targets = mc.get_targets(train_signals, delay, degree=1)\n",
    "test_targets = mc.get_targets(test_signals, delay, degree=1)\n",
    "print(\"train_targets =\", np.around(train_targets, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb64d70",
   "metadata": {},
   "source": [
    "### Step 3: Enconding data into a high dimensional space\n",
    "\n",
    "The following line it should look familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c796a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoirs = list(qrc.reservoir.cptp_map(inputs, washed_reservoir, fermionic_evolution, dim_fermion, excited_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fd3963",
   "metadata": {},
   "source": [
    "Exactly, this function is used to generate a sequence of reservoir states (just as in the wash out step). The imporant thing to remember is that each of these reservoir states contain some information from the past inputs. \n",
    "\n",
    "Now, we would like to extract some information ($x_j$) from the quantum system by performing some measurements.   \n",
    "\n",
    "$$ x_{j}\\left(k\\Delta t\\right)={\\rm Tr}\\left[O_{j}\\rho\\left(u_k\\Delta t\\right)\\right]   \\qquad  (1)$$\n",
    "\n",
    "To do that, we need to define a set of observables. For this tutorial we will use observables of the form $a_i^\\dagger a_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94da0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = qrc.measurements.observables(\"fermion\", n_fermions, dim_fermion, \"ij\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585a761",
   "metadata": {},
   "source": [
    "In the following you can notice that `obs` is a list that contain the diagonal a non-diagonal observables:\n",
    "\n",
    "- $a_0^\\dagger a_0$\n",
    "- $a_1^\\dagger a_1$\n",
    "- $a_0^{\\dagger}a_1 + a_1^{\\dagger}a_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "377eb960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 1.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]\n"
     ]
    }
   ],
   "source": [
    "# diagonal observable: adag_0 a_0\n",
    "print(obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60823ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 1.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 1.+0.j]]\n"
     ]
    }
   ],
   "source": [
    "# diagonal observable: adag_1 a_1 \n",
    "print(obs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "382b44cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 1.+0.j 0.+0.j]\n",
      " [0.+0.j 1.+0.j 0.+0.j 0.+0.j]\n",
      " [0.+0.j 0.+0.j 0.+0.j 0.+0.j]]\n"
     ]
    }
   ],
   "source": [
    "# non-diagonal observable: adag_0 a_1 + adag_1 a_0\n",
    "print(obs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb27be",
   "metadata": {},
   "source": [
    "Typically, conventional neural networks (NN) algorithms use a nonlinear function ($f$) like the ReLU or sigmoid to fit the input data ($x$)\n",
    "\n",
    "$$y_{NN}=f(\\rm{wx}+\\rm{b})$$\n",
    "\n",
    "where $y$ is the output of the NN and $w$, $b$ is the weights and bias that the network will optimize to fit the target data.  \n",
    "\n",
    "With the reservoir computing (RC) framework such a nonlinear function is already introduce in the dynamics of the reservoir. Hence, optimizing the weights becomes an easy task, since they can be found using a linear regression algorithm. \n",
    "\n",
    "$$ y_{RC}=\\rm{wx}+\\rm{b}$$\n",
    "\n",
    "In our case, we have a output ($y_k$) for each input value ($u_k$). So our linear equation is of the form  \n",
    "\n",
    "$$ y_k = \\sum_{j=1}^M \\rm{x_{kj}w_j}$$\n",
    "\n",
    "This can be written in a vectorize notation as:\n",
    "\n",
    "$$ y = XW$$\n",
    "\n",
    "where $y=[y_1, y_2, ... y_k, ... y_L]$ are the predictions of the algorithm, $W=[w_1, w_2, ... w_j, ... w_M, b]$ are the weights and bias to be optimized and $X$ is matrix with shape (L, M+1).\n",
    "\n",
    "$$X=\\begin{bmatrix} {\\rm Tr}\\left[O_{1}\\rho\\left(u_1\\Delta t\\right)\\right]  & {\\rm Tr}\\left[O_{2}\\rho\\left(u_1\\Delta t\\right)\\right] & .. & {\\rm Tr}\\left[O_{M}\\rho\\left(u_1\\Delta t\\right)\\right] & 1 \\\\\n",
    "{\\rm Tr}\\left[O_{1}\\rho\\left(u_2\\Delta t\\right)\\right]  & .. & .. & {\\rm Tr}\\left[O_{M}\\rho\\left(u_2\\Delta t\\right)\\right] & 1\\\\ \n",
    ".. & .. & .. & .. & ..\\\\\n",
    "{\\rm Tr}\\left[O_{1}\\rho\\left(u_L\\Delta t\\right)\\right]  & .. & .. & {\\rm Tr}\\left[O_{M}\\rho\\left(u_L\\Delta t\\right)\\right] & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "To compute the previous matrix, you can use the `get_features` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ae1a939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.039,  0.001,  0.001,  1.   ],\n",
       "       [ 0.443,  0.   ,  0.001,  1.   ],\n",
       "       [ 0.354, -0.002,  0.001,  1.   ],\n",
       "       [ 0.728, -0.004,  0.001,  1.   ],\n",
       "       [ 0.048, -0.001,  0.001,  1.   ],\n",
       "       [ 0.33 ,  0.001,  0.001,  1.   ],\n",
       "       [ 0.333,  0.002,  0.001,  1.   ],\n",
       "       [ 0.989, -0.001,  0.001,  1.   ],\n",
       "       [ 0.147,  0.   ,  0.001,  1.   ],\n",
       "       [ 0.222, -0.002,  0.001,  1.   ],\n",
       "       [ 0.259, -0.003,  0.001,  1.   ],\n",
       "       [ 0.843, -0.002,  0.001,  1.   ],\n",
       "       [ 0.281,  0.001,  0.001,  1.   ],\n",
       "       [ 0.903,  0.   ,  0.001,  1.   ],\n",
       "       [ 0.293,  0.002,  0.001,  1.   ],\n",
       "       [ 0.997, -0.001,  0.001,  1.   ],\n",
       "       [ 0.224, -0.002,  0.001,  1.   ],\n",
       "       [ 0.508, -0.004,  0.001,  1.   ],\n",
       "       [ 0.796, -0.002,  0.001,  1.   ],\n",
       "       [ 0.271,  0.001,  0.001,  1.   ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, _ = qrc.measurements.get_features(reservoirs, obs)\n",
    "\n",
    "# Approximate dataset values up to 3 decimals\n",
    "np.around(dataset, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de76b3",
   "metadata": {},
   "source": [
    "### Step 4: Optimizing weights\n",
    "\n",
    "The optimal weights are computing minimizing the 2-norm error between the target values $\\hat{y}$ and the actual predictions $y=XW$ \n",
    "\n",
    "$$ arg\\min_{W} |\\hat{y}-XW|.$$\n",
    "\n",
    "In the well-known package `scipy` there is a specific function to optimize linear regression problems called `lstsq`. But before we split the dataset into train and test to later evaluate the results and check if the parameters used overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f245c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal weights=[ 2.3400000e-01  2.1895000e+01 -4.8755787e+04  2.8595000e+01]\n"
     ]
    }
   ],
   "source": [
    "train_X = dataset[:train_time, :]\n",
    "test_X = dataset[train_time:, :]\n",
    "\n",
    "# Getting optimal weights\n",
    "opt_weights, _, _, _ = lstsq(train_X, train_targets)\n",
    "\n",
    "print(f\"optimal weights={np.around(opt_weights, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67506a75",
   "metadata": {},
   "source": [
    "### Step 5: Evaluating the memory capacity\n",
    "\n",
    "The memory capacity ($MC$) is a metric that quantifies the linear correlation between two vectors. If both vectors are identical $MC=1$, if there is no linear correlation $MC=0$.\n",
    "\n",
    "$$MC=\\frac{Cov^2(\\hat{y}, y)}{\\sigma^2(\\hat{y})\\sigma^2(y)} $$\n",
    "\n",
    "The algorithm predictions with the best weights are:\n",
    "$$ y_{train}^{\\, p}=X_{train}w_{opt}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8237b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train predictions= [0.701 0.798 0.656 0.601 0.479 0.605 0.733 0.809 0.641 0.543]\n"
     ]
    }
   ],
   "source": [
    "train_predictions = np.matmul(train_X, opt_weights)\n",
    "print(\"train predictions=\", np.around(train_predictions, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc9f1e",
   "metadata": {},
   "source": [
    "We can also compute the predictions on the test dataset `test_X`. \n",
    "$$ y_{pred}^{test} = X_{test}w_{opt}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf2145b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test predictions= [0.436 0.531 0.508 0.688 0.649 0.713 0.442 0.362 0.409 0.404]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = np.matmul(test_X, opt_weights)\n",
    "print(\"test predictions=\", np.around(test_predictions, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336dd05",
   "metadata": {},
   "source": [
    "Finally, we can measure the memory capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c21fbd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MC=0.113\n"
     ]
    }
   ],
   "source": [
    "train_mc = mc.memory_capacity(train_predictions, train_targets)\n",
    "print(f\"Train MC={np.around(train_mc, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34539b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MC=0.015\n"
     ]
    }
   ],
   "source": [
    "test_mc = mc.memory_capacity(test_predictions, test_targets)\n",
    "print(f\"Test MC={np.around(test_mc, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb12215",
   "metadata": {},
   "source": [
    "Cleary, this results by no means great. The memory capacity is quite low and the algorithm is overfitting the data. To get good results the hyperparameters that must be optimized are:\n",
    "\n",
    "- Number of particles \n",
    "- Training data\n",
    "- $\\Delta t$.\n",
    "\n",
    "The Hamiltonian coefficients $J_{ij}$ also play an important role on how the informtion is spread through the system and it will be the next topic we are going to explore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eed7be",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this tutorial, you have learnt how to train a QRC to perform a supervise task. \n",
    "\n",
    "If you want to play around (try different reservoir sizes, particles, train times, etc) we have introudce all the steps shown above in a function called `main`. So, there is no need to re-run previous cells, just set the parameters you are curios about and find out if the performance is improved or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7851d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for fermions \n",
    "\n",
    "# Reservoir parameters\n",
    "n_particles = 4 # 2, 3, .., n\n",
    "dimensions = 2\n",
    "J_ij = qrc.hamiltonian.get_coefficients(n_particles, coef_range=[0, 1], seed=1) # try different seeds\n",
    "operator = \"fermion\"\n",
    "excited_state = 1\n",
    "dt= 10\n",
    "\n",
    "# Performance parameters\n",
    "delay = 1 # 1, 2, .., n\n",
    "obs_form = \"ij\"\n",
    "wash_time = 3000\n",
    "train_time = 1200\n",
    "test_time = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c39d168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir with 4 fermions:\n",
      "\tTrain MC=0.809\n",
      "\tTest  MC=0.808\n"
     ]
    }
   ],
   "source": [
    "_, _, train_mc, test_mc = mc.main(\n",
    "    n_particles,\n",
    "    dimensions,\n",
    "    J_ij,\n",
    "    operator,\n",
    "    delay,\n",
    "    obs_form,\n",
    "    wash_time,\n",
    "    train_time,\n",
    "    test_time,\n",
    "    excited_state,\n",
    "    dt\n",
    ")\n",
    "\n",
    "print(f\"Reservoir with {n_particles} {operator}s:\")\n",
    "print(f\"\\tTrain MC={np.around(train_mc, 3)}\")\n",
    "print(f\"\\tTest  MC={np.around(test_mc, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661bae0",
   "metadata": {},
   "source": [
    "To see the perfomance of spins write `operator=\"spin\"`.  _Remember that as shown in the Figure 2 of the paper the wash time for spins is lower than for fermions_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02c9dfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir with 4 spins:\n",
      "\tTrain MC=0.832\n",
      "\tTest  MC=0.852\n"
     ]
    }
   ],
   "source": [
    "# Spin parameters\n",
    "operator = \"spin\"\n",
    "wash_time = 1000\n",
    "\n",
    "_, _, train_mc, test_mc = mc.main(\n",
    "    n_particles,\n",
    "    dimensions,\n",
    "    J_ij,\n",
    "    operator,\n",
    "    delay,\n",
    "    obs_form,\n",
    "    wash_time,\n",
    "    train_time,\n",
    "    test_time,\n",
    "    excited_state,\n",
    "    dt\n",
    ")\n",
    "\n",
    "print(f\"Reservoir with {n_particles} {operator}s:\")\n",
    "print(f\"\\tTrain MC={np.around(train_mc, 3)}\")\n",
    "print(f\"\\tTest  MC={np.around(test_mc, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5074c22",
   "metadata": {},
   "source": [
    "In particular, for this configuration of parameters spins perform better than fermions. But as shown in our paper fermions will in general have better perfomance than spins. In the last section, of the tutorial we reproduce the Figure 3, in which its shown that fermions outperform spins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035b4ba",
   "metadata": {},
   "source": [
    "## Extra: Bosonic reservoir\n",
    "\n",
    "The main goal of the tutorial was to explain how QRC work. So, in the previous section we explained the basic componets of this machine learning framework using fermions. In this section, there isn't any new concept related to QRC, we will only explain a few key parameters to build a bosonic reservoir.\n",
    "\n",
    "For bosons, there are two new hyperparameters that we can modify: \n",
    "- `dimensions`: For fermions/spins the Hilbert space only have 2 dimensions, but for bosons there are infinite dimensions.\n",
    "- `excited_state`: For bosons we are no restricted to inject information only onto $\\lvert 0 \\rangle$ or $\\lvert 1 \\rangle$. \n",
    "\n",
    "As discussed in the paper to exploit the huge Hilbert space of bosons the input injection must excite high energy levels to get an optimal performance. Using `excited_state=2`the input injection would be of the form:\n",
    "\n",
    "$$ \\lvert \\psi_k^{(e=2)} \\rangle = \\sqrt{u_k}\\lvert 0 \\rangle + \\sqrt{1-u_k}\\lvert 2 \\rangle$$\n",
    "\n",
    "As a drawback (at least for the simulation point of view, not experimentally), as the `excited_state` increases, so it does the number of `dimensions`. As it is shown in Figure 1 for an `excited_state=2` (`excited_state=1`) bosons must have at least `dimensions=6` (`dimensions=5`), in other words 6 (5) energy levels. \n",
    "\n",
    "_If for instance you use `excited_state=2` and `dimensions=3` the particles in the reservoir will be a poor approximation to the actual bosons_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffb4da5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reservoir with 4 bosons:\n",
      "\tTrain MC=0.608\n",
      "\tTest  MC=0.631\n"
     ]
    }
   ],
   "source": [
    "# Boson parameters\n",
    "# This cell can take a few minutes to show the results. And even more if `dimensions=6` and `excited_state=2\n",
    "operator = \"boson\"\n",
    "dimensions = 5\n",
    "excited_state = 1\n",
    "\n",
    "\n",
    "_, _, train_mc, test_mc = mc.main(\n",
    "    n_particles,\n",
    "    dimensions,\n",
    "    J_ij,\n",
    "    operator,\n",
    "    delay,\n",
    "    obs_form,\n",
    "    wash_time,\n",
    "    train_time,\n",
    "    test_time,\n",
    "    excited_state,\n",
    "    dt\n",
    ")\n",
    "\n",
    "print(f\"Reservoir with {n_particles} {operator}s:\")\n",
    "print(f\"\\tTrain MC={np.around(train_mc, 3)}\")\n",
    "print(f\"\\tTest  MC={np.around(test_mc, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8afab2",
   "metadata": {},
   "source": [
    "## Extra: Reproducing Fig. 3\n",
    "\n",
    "\n",
    "Previous results are not statistical relevant since the performance is based one only one realization of $J_{ij}$.  However, it can be a bit time-consuming to run these code from `seed=1`to `seed=1000` to reproduce exactly the same results as in Figure 3. For this reason, we have shared this information in the `data` folder. Now, we will simple upload and plot the results of Figure 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53d36d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(foldername, test_data):\n",
    "    \"\"\"Generates the path to train or test data location\"\"\"\n",
    "    REPO = pathlib.Path().cwd().parent\n",
    "    data = \"test\" if test_data else \"train\"\n",
    "     \n",
    "    return REPO / \"data\" / \"memory_capacity\" / foldername / data\n",
    "\n",
    "def upload_data(foldername, test_data=True):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    data_path = get_path(foldername, test_data)\n",
    "    \n",
    "    # Uploads .npy files in foldername\n",
    "    memory_capacities = list()\n",
    "    for data in data_path.iterdir():\n",
    "        print(str(data).split(\"/\")[11])\n",
    "        memory_capacities.append(np.load(data))\n",
    "    return memory_capacities\n",
    "\n",
    "def rename_folder(foldername, delay):\n",
    "    \"\"\"It changes the delay value from the foldername configuration\"\"\"\n",
    "    split_str = foldername.split(\"_\")\n",
    "    split_str[3]=str(delay)\n",
    "    return \"_\".join(split_str)\n",
    "\n",
    "def plot(x, y, fmt, color, error_bars=None):\n",
    "    if not isinstance(error_bars, type(None)):\n",
    "        error = np.abs(error_bars-y)\n",
    "    \n",
    "    tick, _, _ = plt.errorbar(x, y, yerr=error, fmt=fmt, color=\"black\", ecolor=\"black\",\n",
    "                 elinewidth=1, markersize=9, markerfacecolor=\"black\", markeredgecolor=\"black\")\n",
    "    plt.fill_between(x, error_bars[0], error_bars[1], alpha=0.6, color=color)\n",
    "    plt.grid()\n",
    "    background = mpatches.Patch(color=color, alpha=0.6)\n",
    "    return tick, background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a60aabb1-f488-4e2d-846c-19c31866f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_data in [True, False]:\n",
    "    foldername = \"4_2_fermion_2_ij_3000_1200_300_1_10.0_1_0.0_uniform_(0.0, 1.0)\"\n",
    "    #test_data=True\n",
    "    filename = \"range_1_1000.npy\"\n",
    "\n",
    "    REPO = pathlib.Path().cwd().parent\n",
    "    data = \"test\" if test_data else \"train\"\n",
    "    path = REPO / \"data\" / \"memory_capacity\" / foldername / data / filename\n",
    "\n",
    "    a = upload_data(foldername, test_data)\n",
    "    np.save(str(path), np.stack(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5c8782-edb8-4dac-bfc5-8a6ac473c742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23846570-43e5-4a20-a084-defdae5ccca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43a822c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, delay \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(delays):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Compute the quantile for memory capacities with different delays\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     foldername \u001b[38;5;241m=\u001b[39m rename_folder(foldername, delay)\n\u001b[0;32m---> 11\u001b[0m     test_mc \u001b[38;5;241m=\u001b[39m \u001b[43mupload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfoldername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     q1, q2, q3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mquantile(test_mc, [\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.75\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Store data into the quantiles arrays\u001b[39;00m\n",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36mupload_data\u001b[0;34m(foldername, test_data)\u001b[0m\n\u001b[1;32m     15\u001b[0m memory_capacities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_path\u001b[38;5;241m.\u001b[39miterdir():\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m11\u001b[39m])\n\u001b[1;32m     18\u001b[0m     memory_capacities\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mload(data))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m memory_capacities\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Parameters (change the foldername to upload spin or boson data)\n",
    "delays = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "foldername = \"4_2_fermion_1_ij_3000_1200_300_1_10.0_1_0.0_uniform_(0.0, 1.0)\"\n",
    "test_data = True\n",
    "\n",
    "\n",
    "quantiles_1, quantiles_2, quantiles_3 = np.zeros(len(delays)), np.zeros(len(delays)), np.zeros(len(delays))\n",
    "for idx, delay in enumerate(delays):\n",
    "    # Compute the quantile for memory capacities with different delays\n",
    "    foldername = rename_folder(foldername, delay)\n",
    "    test_mc = upload_data(foldername, test_data)\n",
    "    q1, q2, q3 = np.quantile(test_mc, [0.25, 0.5, 0.75])\n",
    "    \n",
    "    # Store data into the quantiles arrays\n",
    "    quantiles_1[idx] = q1\n",
    "    quantiles_2[idx] = q2\n",
    "    quantiles_3[idx] = q3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d51c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "tick, background = plot(\n",
    "    delays,\n",
    "    quantiles_2, \n",
    "    fmt=\"o--\",\n",
    "    color=\"#1a9641\",\n",
    "    error_bars=np.array([quantiles_1, quantiles_3])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8daf26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "q_reservoir",
   "language": "python",
   "name": "q_reservoir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
